{"HealthLLM": {"2311.09564": "|**2023-11-16**|**LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks**|Mihir Parmar et.al.|[2311.09564](http://arxiv.org/abs/2311.09564)|**[link](https://github.com/mihir3009/longbox)**|\n", "2311.06025": "|**2023-11-10**|**ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences**|Yuanhe Tian et.al.|[2311.06025](http://arxiv.org/abs/2311.06025)|**[link](https://github.com/synlp/chimed-gpt)**|\n", "2311.05112": "|**2023-11-09**|**A Survey of Large Language Models in Medicine: Progress, Application, and Challenge**|Hongjian Zhou et.al.|[2311.05112](http://arxiv.org/abs/2311.05112)|**[link](https://github.com/ai-in-health/medllmspracticalguide)**|\n", "2310.15896": "|**2023-10-24**|**BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT**|Yirong Chen et.al.|[2310.15896](http://arxiv.org/abs/2310.15896)|**[link](https://github.com/scutcyr/bianque)**|\n", "2310.14151": "|**2023-10-22**|**PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain**|Wei Zhu et.al.|[2310.14151](http://arxiv.org/abs/2310.14151)|**[link](https://github.com/michael-wzhu/PromptCBLUE)**|\n", "2310.10083": "|**2023-10-16**|**JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning**|Issey Sukeda et.al.|[2310.10083](http://arxiv.org/abs/2310.10083)|null|\n", "2310.09089": "|**2023-10-13**|**Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model**|Qichen Ye et.al.|[2310.09089](http://arxiv.org/abs/2310.09089)|**[link](https://github.com/williamliujl/Qilin-Med)**|\n", "2308.14346": "|**2023-08-28**|**DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation**|Zhijie Bao et.al.|[2308.14346](http://arxiv.org/abs/2308.14346)|**[link](https://github.com/fudandisc/disc-medllm)**|\n", "2308.08833": "|**2023-08-17**|**CMB: A Comprehensive Medical Benchmark in Chinese**|Xidong Wang et.al.|[2308.08833](http://arxiv.org/abs/2308.08833)|**[link](https://github.com/FreedomIntelligence/CMB)**|\n", "2308.07635": "|**2023-08-15**|**LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation**|Xiaoming Shi et.al.|[2308.07635](http://arxiv.org/abs/2308.07635)|null|\n", "2307.10188": "|**2023-07-05**|**Several categories of Large Language Models (LLMs): A Short Survey**|Saurabh Pahune et.al.|[2307.10188](http://arxiv.org/abs/2307.10188)|null|\n", "2306.10765": "|**2023-06-19**|**Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost**|Juexiao Zhou et.al.|[2306.10765](http://arxiv.org/abs/2306.10765)|**[link](https://github.com/joshuachou2018/medagi)**|\n", "2305.12031": "|**2023-08-17**|**Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding**|Augustin Toma et.al.|[2305.12031](http://arxiv.org/abs/2305.12031)|**[link](https://github.com/bowang-lab/clinical-camel)**|\n", "2303.14070": "|**2023-06-24**|**ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge**|Yunxiang Li et.al.|[2303.14070](http://arxiv.org/abs/2303.14070)|**[link](https://github.com/kent0n-li/chatdoctor)**|\n", "2309.00237": "|**2023-09-06**|**Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes**|Sunjun Kweon et.al.|[2309.00237](http://arxiv.org/abs/2309.00237)|**[link](https://github.com/starmpcc/asclepius)**|\n", "2307.09018": "|**2023-07-20**|**Multimodal LLMs for health grounded in individual-specific data**|Anastasiya Belyaeva et.al.|[2307.09018](http://arxiv.org/abs/2307.09018)|null|\n", "2303.08448": "|**2023-03-15**|**A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records**|Sicheng Zhou et.al.|[2303.08448](http://arxiv.org/abs/2303.08448)|null|\n"}}